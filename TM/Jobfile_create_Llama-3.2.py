import torch
import os
import json
import shutil
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.nn as nn
import torch.optim as optim
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# âœ… 1. ëª¨ë¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
model_name = "meta-llama/Llama-3.2-3B-Instruct"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
HUGGINGFACE_TOKEN = "test_token"

# âœ… Fine-Tuned Llama ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGINGFACE_TOKEN)
llama_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    token=HUGGINGFACE_TOKEN
).to(device)

# âœ… í† í¬ë‚˜ì´ì €ì— pad_token ì¶”ê°€ (í•„ìˆ˜)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    llama_model.resize_token_embeddings(len(tokenizer))

# âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ì„¤ì •
db_path = "faiss_robot_jobfile_db"

# âœ… 2. FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
if not os.path.exists(db_path):
    print("ğŸ”¹ FAISS ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ. ìƒˆë¡œ ìƒì„± ì¤‘...")

    # âœ… Job File í…œí”Œë¦¿ ë°ì´í„° ë¡œë“œ
    jobfile_templates = [
    # âœ… H2017 Job File ì˜ˆì œ
        {
            "model": "H2017",
            "description": "Doosan Robotics í˜‘ë™ë¡œë´‡ H2017ì˜ Task Editor ê¸°ë°˜ JSON Job File í¬ë§·",
            "jobfile": json.dumps({
                "application": "task-editor",
                "data": {
                    "project": {
                        "name": "Task_Example",
                        "model": "H2017",
                        "fileParameter": "robot-param-01",
                        "createDate": "2025.02.17 16:00:05",
                        "updateDate": "2025.02.17 16:50:02"
                    },
                    "taskList": [
                        {
                            "id": "123456",
                            "type": "move_periodic",
                            "name": "Move Periodic",
                            "properties": {
                                "coordinates": 0,
                                "operatingMode": "sync",
                                "operationCondition": "amplitude",
                                "repeatNumber": 1,
                                "amplitude": { "x": 10, "y": 20, "z": 30, "a": 0, "b": 0, "c": 0 },
                                "period": { "x": 5, "y": 5, "z": 5, "a": 0, "b": 0, "c": 0 }
                            }
                        }
                    ]
                }
            }, indent=2)
        },
        # âœ… HS220 Job File ì˜ˆì œ
        {
            "model": "HS220",
            "description": "Yaskawa HS220 ì‚°ì—…ìš© ë¡œë´‡ì˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ Job File í¬ë§·",
            "jobfile": """
            Program File Format Version : 1.6  MechType: 693(HS220-03)  TotalAxis: 6  AuxAxis: 0
            S1   MOVE P,S=60%,A=3,T=1  (15.098,76.218,-23.05,-327.989,40.853,241.733)A
            S2   MOVE P,S=60%,A=3,T=1  (13.064,78.627,-23.826,-325.036,41.168,238.127)A
            S3   MOVE P,S=60%,A=3,T=1  (11.035,81.026,-24.591,-322.088,41.478,234.515)A
            S4   MOVE P,S=60%,A=3,T=1  (8.995,83.434,-25.361,-319.142,41.795,230.911)A
            END
            """
        }
    ]

    # âœ… JSON íŒŒì¼ë¡œ ì €ì¥
    with open("robot_jobfile_templates.json", "w", encoding="utf-8") as f:
        json.dump({"jobfile_templates": jobfile_templates}, f, indent=4)

    # âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    documents = [f"{entry['model']}: {entry['description']}\nExample: {entry['jobfile']}" for entry in jobfile_templates]
    vector_db = FAISS.from_texts(documents, embedding_model)
    vector_db.save_local(db_path)
    print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
else:
    # âœ… ê¸°ì¡´ FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

# âœ… 3. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì²˜ë¦¬
dataset_path = "robot_jobfile_dataset.json"

if not os.path.exists(dataset_path):
    # âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    jobfile_dataset = {
        "jobfile_pairs": [
            {
                "text": "Generate a job file for H2017 robotic arm with basic movement commands.",
                "label": "The best job file format for H2017 is: {'application':'task-editor','data':{'project':{'name':'Task_H2017', 'model':'H2017'}, 'taskList':[...]}}"
            },
            {
                "text": "Create a job file for HS220 robotic arm for movement sequence.",
                "label": "The best job file format for HS220 is: Program File Format Version : 1.6  MechType: 693(HS220-03)  TotalAxis: 6  AuxAxis: 0 S1 MOVE P,S=60%,A=3,T=1 (15.098,76.218,-23.05,-327.989,40.853,241.733)A ..."
            }
        ]
    }
    with open(dataset_path, "w", encoding="utf-8") as f:
        json.dump(jobfile_dataset, f, indent=4)

# âœ… JSON íŒŒì¼ ë¡œë“œ
with open(dataset_path, "r", encoding="utf-8") as f:
    dataset_json = json.load(f)

data = dataset_json["jobfile_pairs"]

# âœ… 4. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class JobFileDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # âœ… RAG ê²€ìƒ‰: ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = vector_db.similarity_search(item["text"], k=1)
        context = retrieved_docs[0].page_content if retrieved_docs else ""

        # âœ… ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
        input_text = f"Context: {context}\n\n{item['text']}"
        output_text = f"{item['label']}"

        input_ids = tokenizer(input_text, return_tensors="pt", padding="max_length", max_length=self.max_length, truncation=True)
        label_ids = tokenizer(output_text, return_tensors="pt", padding="max_length", max_length=self.max_length, truncation=True)

        labels = label_ids["input_ids"].squeeze(0)
        labels[labels == tokenizer.pad_token_id] = -100  # ğŸ”¹ Padding ë¬´ì‹œ

        return {"input_ids": input_ids["input_ids"].squeeze(0), "labels": labels}

# âœ… ë°ì´í„°ì…‹ ë° DataLoader ì„¤ì •
dataset = JobFileDataset(data, tokenizer, max_length=512)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™” ê¸°ë²• ì •ì˜
criterion = nn.CrossEntropyLoss(ignore_index=-100)
optimizer = optim.AdamW(llama_model.parameters(), lr=2e-5)

# âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•¨ìˆ˜
def save_checkpoint(epoch):
    checkpoint_path = f"./checkpoint_epoch_{epoch}.pt"
    torch.save({
        'epoch': epoch,
        'model_state_dict': llama_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss
    }, checkpoint_path)
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {checkpoint_path}")

# âœ… í•™ìŠµ ë£¨í”„
num_epochs = 5
llama_model.train()

for epoch in range(num_epochs):
    total_loss = 0
    for batch in dataloader:
        inputs = {k: v.to(device) for k, v in batch.items()}

        optimizer.zero_grad()
        outputs = llama_model(input_ids=inputs["input_ids"], labels=inputs["labels"])
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}")
    save_checkpoint(epoch)

# âœ… ëª¨ë¸ ì €ì¥
save_path = "./fine_tuned_llama_jobfile"
llama_model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("âœ… Fine-Tuning ì™„ë£Œ ë° ëª¨ë¸ ì €ì¥!")
