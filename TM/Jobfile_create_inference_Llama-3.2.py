import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# âœ… ëª¨ë¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
model_path = "./fine_tuned_llama_jobfile"
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… Fine-Tuned ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path)
llama_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
).to(device)

# âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
db_path = "faiss_robot_jobfile_db"
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)

print("âœ… ëª¨ë¸ ë° FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

# âœ… 1. ì‚¬ìš©ì ì…ë ¥
robot_model = input("ğŸ“ Enter the robot model (e.g., H2017, HS220): ").strip()
command_description = input("ğŸ“ Describe the movement sequence: ").strip()

# âœ… 2. FAISS RAG ê²€ìƒ‰ (ìœ ì‚¬í•œ Job File ê²€ìƒ‰)
query = f"{robot_model}: {command_description}"
retrieved_docs = vector_db.similarity_search(query, k=1)

if retrieved_docs:
    context = retrieved_docs[0].page_content
else:
    context = "No relevant job file found. Provide the best possible output."

# âœ… 3. ëª¨ë¸ ì…ë ¥ êµ¬ì„±
combined_prompt = f"Context: {context}\n\nGenerate a job file for {robot_model} with the following command: {command_description}"

# âœ… 4. í† í¬ë‚˜ì´ì§•
inputs = tokenizer(
    combined_prompt,
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=512
).to(device)

# âœ… 5. ëª¨ë¸ ì¶”ë¡  (Job File ìƒì„±)
print("\nğŸ”¹ Generating Job File...\n")

with torch.no_grad():
    output_tokens = llama_model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.pad_token_id
    )

jobfile_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# âœ… 6. ê²°ê³¼ ì¶œë ¥
print("\nğŸ”¹ **Generated Job File:**")
print(jobfile_response)
