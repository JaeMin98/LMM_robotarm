import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings


class LlamaGripperPredictor:
    """
    Fine-Tuned LLaMA ëª¨ë¸ê³¼ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ìš©í•´
    ê°ì²´ ì„¤ëª…ì— ëŒ€í•´ ê·¸ë¦¬í¼ë¥¼ ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, model_path: str, faiss_db_path: str, device: str = None):
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = device if device is not None else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model_path = model_path
        self.faiss_db_path = faiss_db_path

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("âœ… Fine-Tuned LLaMA ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        ).to(self.device)
        print("âœ… Fine-Tuned LLaMA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

        # FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
        embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        self.vector_db = FAISS.load_local(self.faiss_db_path, embedding_model, allow_dangerous_deserialization=True)
        print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

        # ìœ íš¨í•œ ê·¸ë¦¬í¼ ëª©ë¡ ë° ì •ê·œì‹ íŒ¨í„´ ìƒì„±
        self.valid_grippers = ["Suction Gripper", "Parallel Jaw Gripper", "Magnetic Gripper"]
        self.gripper_pattern = "|".join(map(re.escape, self.valid_grippers))

    def retrieve_context(self, description: str) -> str:
        retrieved_docs = self.vector_db.similarity_search(description, k=1)
        context = retrieved_docs[0].page_content if retrieved_docs else ""
        return context

    def generate_response(self, description: str, context: str, max_length: int = 256, max_new_tokens: int = 100) -> str:
        combined_prompt = f"Context: {context}\n\n{description}"
        inputs = self.tokenizer(
            combined_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(self.device)

        print("ğŸ”¹ Processing text input...")

        with torch.no_grad():
            output_tokens = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id
            )

        response = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
        return response

    def extract_recommended_gripper(self, response: str) -> str:
        """
        ëª¨ë¸ì˜ ì‘ë‹µì—ì„œ ì¶”ì²œëœ ê·¸ë¦¬í¼ë¥¼ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        match = re.search(self.gripper_pattern, response)
        return match.group(0) if match else "Unknown"

    def predict(self, description: str) -> dict:
        """
        ì „ì²´ ì˜ˆì¸¡ ê³¼ì •ì„ ìˆ˜í–‰í•˜ì—¬ ì¶”ì²œ ê·¸ë¦¬í¼, ëª¨ë¸ ì‘ë‹µ, ê²€ìƒ‰ ë¬¸ë§¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        context = self.retrieve_context(description)
        response = self.generate_response(description, context)
        recommended_gripper = self.extract_recommended_gripper(response)
        return {
            "description": description,
            "recommended_gripper": recommended_gripper,
            "explanation": response,
            "context": context
        }


class IOHandler:
    """
    ì‚¬ìš©ì ì…ë ¥ê³¼ ì¶œë ¥ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤.
    """
    @staticmethod
    def get_user_input(prompt: str = "ğŸ“ Enter a description of the object: ") -> str:
        """
        ì‚¬ìš©ìë¡œë¶€í„° ê°ì²´ ì„¤ëª…ì„ ì…ë ¥ ë°›ê¸°
        """
        return input(prompt).strip()

    @staticmethod
    def print_prediction(result: dict):
        """
        ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¶œë ¥
        """
        print("\nğŸ”¹ **LLaMA Model Prediction:**")
        print(f"ğŸ“ Object Description: {result['description']}")
        print(f"âœ… Recommended Gripper: {result['recommended_gripper']}")
        print(f"ğŸ’¡ Explanation: {result['explanation']}")
        print("\nğŸ”¹ **Retrieved Context from FAISS:**")
        print(result['context'])


def main():
    # ì„¤ì •: ëª¨ë¸ ê²½ë¡œ, FAISS ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ, ë””ë°”ì´ìŠ¤ ì„¤ì •
    model_path = "./fine_tuned_llama_text"
    faiss_db_path = "faiss_gripper_db"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # ì˜ˆì¸¡ê¸° ê°ì²´ ìƒì„±
    predictor = LlamaGripperPredictor(model_path, faiss_db_path, device)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_description = IOHandler.get_user_input()

    # ì˜ˆì¸¡ ìˆ˜í–‰
    result = predictor.predict(user_description)

    # ê²°ê³¼ ì¶œë ¥
    IOHandler.print_prediction(result)


if __name__ == "__main__":
    main()
