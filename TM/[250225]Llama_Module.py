import os
import json
import torch
import shutil
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings


def load_model_and_tokenizer(model_name: str, token: str, device: torch.device):
    """
    ì£¼ì–´ì§„ ëª¨ë¸ ì´ë¦„ê³¼ Hugging Face í† í°ì„ ì‚¬ìš©í•´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        token=token
    ).to(device)
    return model, tokenizer


def prepare_tokenizer(tokenizer, model):
    """
    í† í¬ë‚˜ì´ì €ì— pad_tokenì´ ì—†ìœ¼ë©´ ì¶”ê°€í•˜ê³ , ëª¨ë¸ì˜ í† í° ì„ë² ë”© í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
    """
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        model.resize_token_embeddings(len(tokenizer))


def load_or_create_faiss_db(db_path: str):
    """
    FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ì£¼ì–´ì§„ Gripper ë¬¸ì„œë“¤ì„ ì‚¬ìš©í•´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    if not os.path.exists(db_path):
        print("ğŸ”¹ FAISS ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ. ìƒˆë¡œ ìƒì„± ì¤‘...")
        documents = [
            "A Suction Gripper is best for soft, round objects like apples. It uses vacuum suction.",
            "A Parallel Jaw Gripper is best for flat, rigid objects like boxes. It uses mechanical clamping.",
            "A Magnetic Gripper is best for metallic objects like rods. It uses magnetism.",
            "A Suction Gripper works well on lightweight, deformable objects like foam balls.",
            "Parallel Jaw Grippers are ideal for securely grasping paper stacks without damaging them.",
            "Magnetic Grippers are commonly used in industrial settings to manipulate metal sheets and rods."
        ]
        vector_db = FAISS.from_texts(documents, embedding_model)
        vector_db.save_local(db_path)
        print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    else:
        vector_db = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
        print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
    return vector_db


def load_dataset(dataset_path: str):
    """
    JSON íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤. 'image_text_pairs' í‚¤ì˜ ì¡´ì¬ì™€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì„ í™•ì¸í•©ë‹ˆë‹¤.
    """
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset_json = json.load(f)

    if "image_text_pairs" not in dataset_json:
        raise KeyError("âŒ 'image_text_pairs' keyê°€ JSON íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

    data = dataset_json["image_text_pairs"]
    if not isinstance(data, list):
        raise ValueError(f"âŒ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {type(data)}")
    return data


class GripperDataset(Dataset):
    """
    Gripper ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ìŒ ë°ì´í„°ì…‹ í´ë˜ìŠ¤.
    RAG ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ FAISS ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ ì…ë ¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    def __init__(self, data, tokenizer, vector_db, max_length=256):
        self.data = data
        self.tokenizer = tokenizer
        self.vector_db = vector_db
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if idx >= len(self.data):
            raise IndexError(f"âŒ ì˜ëª»ëœ ì¸ë±ìŠ¤ ì ‘ê·¼: {idx} (ë°ì´í„° ê¸¸ì´: {len(self.data)})")

        item = self.data[idx]
        if "text" not in item or "label" not in item:
            raise KeyError(f"âŒ ë°ì´í„°ì— 'text' ë˜ëŠ” 'label' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {item}")

        # RAG ê²€ìƒ‰: ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰
        retrieved_docs = self.vector_db.similarity_search(item["text"], k=1)
        context = retrieved_docs[0].page_content if retrieved_docs else ""
        input_text = f"Context: {context}\n\n{item['text']}"
        output_text = f"{item['label']}"

        # ì…ë ¥ê³¼ ì •ë‹µ í† í°í™” (ìµœëŒ€ ê¸¸ì´, padding, truncation ì ìš©)
        input_ids = self.tokenizer(
            input_text,
            return_tensors="pt",
            padding="max_length",
            max_length=self.max_length,
            truncation=True
        )

        label_ids = self.tokenizer(
            output_text,
            return_tensors="pt",
            padding="max_length",
            max_length=self.max_length,
            truncation=True
        )

        labels = label_ids["input_ids"].squeeze(0)
        # ì†ì‹¤ ê³„ì‚° ì‹œ íŒ¨ë”© í† í°ì€ ë¬´ì‹œ
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "input_ids": input_ids["input_ids"].squeeze(0),
            "labels": labels
        }


class LlamaFineTuner:
    """
    Llama ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ì„ ìœ„í•œ í´ë˜ìŠ¤.
    í•™ìŠµ ë£¨í”„, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”, ëª¨ë¸ ì €ì¥ ë“±ì˜ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    def __init__(self, model, tokenizer, dataloader, device, lr=2e-5, num_epochs=3):
        self.model = model
        self.tokenizer = tokenizer
        self.dataloader = dataloader
        self.device = device
        self.lr = lr
        self.num_epochs = num_epochs
        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

    def train(self):
        self.model.train()
        for epoch in range(self.num_epochs):
            total_loss = 0
            for batch in self.dataloader:
                # ëª¨ë“  í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                inputs = {k: v.to(self.device) for k, v in batch.items()}
                self.optimizer.zero_grad()
                outputs = self.model(
                    input_ids=inputs["input_ids"],
                    labels=inputs["labels"]
                )
                loss = outputs.loss
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{self.num_epochs}, Loss: {total_loss:.4f}")

    def save_model(self, save_path: str):
        """
        íŒŒì¸íŠœë‹ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì •í•œ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        print("âœ… Fine-Tuning ì™„ë£Œ ë° ëª¨ë¸ ì €ì¥!")


def main():
    # 1. ëª¨ë¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
    model_name = "meta-llama/Llama-3.2-3B-Instruct"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    HUGGINGFACE_TOKEN = "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"

    model, tokenizer = load_model_and_tokenizer(model_name, HUGGINGFACE_TOKEN, device)
    prepare_tokenizer(tokenizer, model)

    # 2. FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë˜ëŠ” ë¡œë“œ
    db_path = "faiss_gripper_db"
    vector_db = load_or_create_faiss_db(db_path)

    # 3. ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_path = "GripperImages/gripper_dataset.json"
    data = load_dataset(dataset_path)

    # 4. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë° DataLoader ìƒì„±
    gripper_dataset = GripperDataset(data, tokenizer, vector_db, max_length=256)
    dataloader = DataLoader(gripper_dataset, batch_size=1, shuffle=True)

    # 5. ëª¨ë¸ íŒŒì¸íŠœë‹ ìˆ˜í–‰
    fine_tuner = LlamaFineTuner(model, tokenizer, dataloader, device, lr=2e-5, num_epochs=3)
    fine_tuner.train()

    # 6. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥
    save_path = "./fine_tuned_llama_text"
    fine_tuner.save_model(save_path)


if __name__ == "__main__":
    main()
