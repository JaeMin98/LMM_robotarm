import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings


class ModelManager:
    @staticmethod
    def load_model_and_tokenizer(model_name: str, token: str, device: torch.device):
        """
        ì£¼ì–´ì§„ ëª¨ë¸ ì´ë¦„ê³¼ HF í† í°ì„ ì‚¬ìš©í•´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            token=token
        ).to(device)
        return model, tokenizer

    @staticmethod
    def prepare_tokenizer(tokenizer, model):
        """
        í† í¬ë‚˜ì´ì €ì— pad_tokenì´ ì—†ìœ¼ë©´ ì¶”ê°€í•˜ê³ , ëª¨ë¸ì˜ í† í° ì„ë² ë”© í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
        """
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            model.resize_token_embeddings(len(tokenizer))


class FAISSManager:
    @staticmethod
    def setup_faiss_db(db_path: str) -> FAISS:
        """
        ì§€ì •ëœ ê²½ë¡œì— FAISS ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ Job File í…œí”Œë¦¿ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•˜ê³ ,
        ì¡´ì¬í•˜ë©´ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        if not os.path.exists(db_path):
            print("ğŸ”¹ FAISS ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ. ìƒˆë¡œ ìƒì„± ì¤‘...")
            # Job File í…œí”Œë¦¿ ë°ì´í„° ì •ì˜
            jobfile_templates = [
                # H2017 - Suction Gripper
                {
                    "model": "H2017",
                    "gripper": "Suction Gripper",
                    "description": "Suction Gripperë¥¼ ì¥ì°©í•˜ëŠ” Job File.",
                    "jobfile": json.dumps({
                        "task": "Attach Suction Gripper",
                        "steps": [
                            {"action": "MOVEJ", "S": 50, "A": 3, "T": 2, "coordinates": [10, 20, 30, 0, 0, 0]},
                            {"action": "ATTACH", "gripper": "Suction Gripper"},
                            {"action": "MOVEJ", "S": 50, "A": 3, "T": 2, "coordinates": [0, 0, 0, 0, 0, 0]}
                        ]
                    }, indent=2)
                },
                # H2017 - Parallel Jaw Gripper
                {
                    "model": "H2017",
                    "gripper": "Parallel Jaw Gripper",
                    "description": "Parallel Jaw Gripperë¥¼ ì¥ì°©í•˜ëŠ” Job File.",
                    "jobfile": json.dumps({
                        "task": "Attach Parallel Jaw Gripper",
                        "steps": [
                            {"action": "MOVEL", "S": 30, "A": 2, "T": 1, "coordinates": [15, 25, 35, 0, 0, 0]},
                            {"action": "ATTACH", "gripper": "Parallel Jaw Gripper"},
                            {"action": "MOVEL", "S": 30, "A": 2, "T": 1, "coordinates": [0, 0, 0, 0, 0, 0]}
                        ]
                    }, indent=2)
                },
                # H2017 - Magnetic Gripper
                {
                    "model": "H2017",
                    "gripper": "Magnetic Gripper",
                    "description": "Magnetic Gripperë¥¼ ì¥ì°©í•˜ëŠ” Job File.",
                    "jobfile": json.dumps({
                        "task": "Attach Magnetic Gripper",
                        "steps": [
                            {"action": "MOVEP", "S": 40, "A": 2, "T": 1, "coordinates": [12, 18, 28, 0, 0, 0]},
                            {"action": "ATTACH", "gripper": "Magnetic Gripper"},
                            {"action": "MOVEP", "S": 40, "A": 2, "T": 1, "coordinates": [0, 0, 0, 0, 0, 0]}
                        ]
                    }, indent=2)
                },
                # HS220 - Suction Gripper
                {
                    "model": "HS220",
                    "gripper": "Suction Gripper",
                    "description": "Suction Gripperë¥¼ ì¥ì°©í•˜ëŠ” Job File.",
                    "jobfile": """
            Program File Format Version : 1.6  MechType: 693(HS220-03)  TotalAxis: 6  AuxAxis: 0
            S1   MOVE P,S=60%,A=3,T=1  (10.5, 20.3, -5.7, -327.5, 40.2, 241.0)A
            S2   ATTACH Suction Gripper
            S3   MOVE P,S=60%,A=3,T=1  (0, 0, 0, 0, 0, 0)A
            END
                    """
                },
                # HS220 - Parallel Jaw Gripper
                {
                    "model": "HS220",
                    "gripper": "Parallel Jaw Gripper",
                    "description": "Parallel Jaw Gripperë¥¼ ì¥ì°©í•˜ëŠ” Job File.",
                    "jobfile": """
            Program File Format Version : 1.6  MechType: 693(HS220-03)  TotalAxis: 6  AuxAxis: 0
            S1   MOVE P,S=60%,A=3,T=1  (14.3, 22.8, -6.2, -320.7, 38.5, 237.2)A
            S2   ATTACH Parallel Jaw Gripper
            S3   MOVE P,S=60%,A=3,T=1  (0, 0, 0, 0, 0, 0)A
            END
                    """
                },
                # HS220 - Magnetic Gripper
                {
                    "model": "HS220",
                    "gripper": "Magnetic Gripper",
                    "description": "Magnetic Gripperë¥¼ ì¥ì°©í•˜ëŠ” Job File.",
                    "jobfile": """
            Program File Format Version : 1.6  MechType: 693(HS220-03)  TotalAxis: 6  AuxAxis: 0
            S1   MOVE P,S=60%,A=3,T=1  (18.0, 30.1, -8.5, -312.4, 35.7, 232.5)A
            S2   ATTACH Magnetic Gripper
            S3   MOVE P,S=60%,A=3,T=1  (0, 0, 0, 0, 0, 0)A
            END
                    """
                }
            ]

            # Job File í…œí”Œë¦¿ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
            with open("robot_jobfile_templates.json", "w", encoding="utf-8") as f:
                json.dump({"jobfile_templates": jobfile_templates}, f, indent=4)

            # ê° í…œí”Œë¦¿ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©
            documents = [
                f"{entry['model']}: {entry['description']}\n{entry['jobfile']}"
                for entry in jobfile_templates
            ]
            vector_db = FAISS.from_texts(documents, embedding_model)
            vector_db.save_local(db_path)
            print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
        else:
            vector_db = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
            print("âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
        return vector_db


class JobFileDataset(Dataset):
    """
    íŒŒì¸íŠœë‹ìš© Job File ë°ì´í„°ì…‹.
    ê° ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•´ FAISSë¥¼ í™œìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ë§¥(context)ì„ ê²€ìƒ‰í•˜ê³ ,
    ì´ë¥¼ ì…ë ¥ í…ìŠ¤íŠ¸ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
    """
    def __init__(self, data, tokenizer, vector_db, max_length: int = 512):
        self.data = data
        self.tokenizer = tokenizer
        self.vector_db = vector_db
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        item = self.data[idx]

        # FAISS ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰
        retrieved_docs = self.vector_db.similarity_search(item["text"], k=1)
        context = retrieved_docs[0].page_content if retrieved_docs else ""

        # ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±: ê²€ìƒ‰ëœ FAISS ë¬¸ë§¥ì„ Contextë¡œ í™œìš©
        input_text = f"Context: {context}\n\n{item['text']}"
        output_text = item["label"]

        # í† í¬ë‚˜ì´ì§• (padding, truncation, max_length ì ìš©)
        input_ids = self.tokenizer(
            input_text,
            return_tensors="pt",
            padding="max_length",
            max_length=self.max_length,
            truncation=True
        )
        label_ids = self.tokenizer(
            output_text,
            return_tensors="pt",
            padding="max_length",
            max_length=self.max_length,
            truncation=True
        )
        labels = label_ids["input_ids"].squeeze(0)
        # íŒ¨ë”© í† í°ì€ ì†ì‹¤ ê³„ì‚° ì‹œ ë¬´ì‹œ (-100)
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "input_ids": input_ids["input_ids"].squeeze(0),
            "labels": labels
        }


class LlamaJobFileFineTuner:
    """
    íŒŒì¸íŠœë‹ì„ ìœ„í•œ í´ë˜ìŠ¤.
    ëª¨ë¸ í•™ìŠµ, ì˜µí‹°ë§ˆì´ì € ì„¤ì •, ì†ì‹¤ ê³„ì‚° ë° ëª¨ë¸ ì €ì¥ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    def __init__(self, model, tokenizer, dataloader, device, lr: float = 2e-5, num_epochs: int = 5):
        self.model = model
        self.tokenizer = tokenizer
        self.dataloader = dataloader
        self.device = device
        self.lr = lr
        self.num_epochs = num_epochs
        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

    def train(self):
        self.model.train()
        for epoch in range(self.num_epochs):
            total_loss = 0
            for batch in self.dataloader:
                inputs = {k: v.to(self.device) for k, v in batch.items()}
                self.optimizer.zero_grad()
                outputs = self.model(input_ids=inputs["input_ids"], labels=inputs["labels"])
                loss = outputs.loss
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{self.num_epochs}, Loss: {total_loss:.4f}")

    def save_model(self, save_path: str):
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        print("âœ… Fine-Tuning ì™„ë£Œ ë° ëª¨ë¸ ì €ì¥!")


def load_dataset(dataset_path: str) -> list:
    """
    ì£¼ì–´ì§„ JSON íŒŒì¼ì—ì„œ 'jobfile_pairs' í‚¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    if not os.path.exists(dataset_path):
        print("âŒ ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        exit()
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset_json = json.load(f)
    data = dataset_json["jobfile_pairs"]
    return data


def main():
    # 1. ëª¨ë¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
    model_name = "meta-llama/Llama-3.2-3B-Instruct"
    HUGGINGFACE_TOKEN = "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"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model, tokenizer = ModelManager.load_model_and_tokenizer(model_name, HUGGINGFACE_TOKEN, device)
    ModelManager.prepare_tokenizer(tokenizer, model)

    # 2. FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë˜ëŠ” ë¡œë“œ (Job File í…œí”Œë¦¿ ê¸°ë°˜)
    db_path = "faiss_robot_gripper_jobfile_db"
    vector_db = FAISSManager.setup_faiss_db(db_path)

    # 3. íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_path = "robot_jobfile_dataset.json"
    data = load_dataset(dataset_path)

    # 4. Dataset ë° DataLoader êµ¬ì„±
    dataset = JobFileDataset(data, tokenizer, vector_db, max_length=512)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

    # 5. í•™ìŠµ ë£¨í”„ ìˆ˜í–‰
    finetuner = LlamaJobFileFineTuner(model, tokenizer, dataloader, device, lr=2e-5, num_epochs=5)
    finetuner.train()

    # 6. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥
    save_path = "./fine_tuned_llama_gripper_jobfile"
    finetuner.save_model(save_path)


if __name__ == "__main__":
    main()
