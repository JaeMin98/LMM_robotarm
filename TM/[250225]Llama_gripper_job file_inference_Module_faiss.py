import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from typing import Tuple


class JobFileGenerator:
    """
    ëª¨ë¸, í† í¬ë‚˜ì´ì € ë° FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•˜ê³ ,
    ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¥¸ Job Fileì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, model_path: str, db_path: str, device: str = None) -> None:
        self.model_path = model_path
        self.device = device if device is not None else ("cuda" if torch.cuda.is_available() else "cpu")
        
        # Fine-Tuned ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        ).to(self.device)
        
        # FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        self.embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        if not os.path.exists(db_path):
            print("âŒ FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            exit()
        self.vector_db = FAISS.load_local(db_path, self.embedding_model, allow_dangerous_deserialization=True)
        print("âœ… ëª¨ë¸ ë° FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
        
        # ì…ë ¥ê°’ ê²€ì¦ì„ ìœ„í•œ ê¸°ì¤€ ì„¤ì •
        self.robot_models = ["H2017", "HS220"]
        self.gripper_mapping = {
            "A": "Suction Gripper",
            "B": "Parallel Jaw Gripper",
            "C": "Magnetic Gripper"
        }

    def validate_inputs(self, robot_model: str, gripper_type: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ê°’(ë¡œë´‡ ëª¨ë¸, ê·¸ë¦¬í¼ íƒ€ì…)ì„ ê²€ì¦í•˜ê³ ,
        ê·¸ë¦¬í¼ íƒ€ì…ì— ëŒ€ì‘í•˜ëŠ” ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if robot_model not in self.robot_models:
            raise ValueError(f"âŒ Error: Invalid robot model '{robot_model}'. Please choose 'H2017' or 'HS220'.")
        if gripper_type not in self.gripper_mapping:
            raise ValueError(f"âŒ Error: Invalid gripper type '{gripper_type}'. Please choose 'A', 'B', or 'C'.")
        return self.gripper_mapping[gripper_type]

    def retrieve_context(self, robot_model: str, gripper_name: str) -> str:
        """
        FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì…ë ¥ê°’ê³¼ ìœ ì‚¬í•œ Job File ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        query = f"{robot_model}: {gripper_name} Gripper Installation"
        retrieved_docs = self.vector_db.similarity_search(query, k=1)
        if retrieved_docs:
            return retrieved_docs[0].page_content
        else:
            return "No relevant job file found. Provide the best possible output."

    def generate_prompt(self, robot_model: str, gripper_name: str, context: str) -> str:
        """
        Job File ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        prompt = f"""
Robot Model: {robot_model}
Gripper Type: {gripper_name}

Context: {context}
"""
        return prompt

    def generate_job_file(
        self, 
        robot_model: str, 
        gripper_type: str, 
        max_new_tokens: int = 1, 
        temperature: float = 0.7,
        top_p: float = 0.9, 
        max_length: int = 512
    ) -> Tuple[str, str]:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ Job Fileì„ ìƒì„±í•˜ê³ ,
        ê·¸ë¦¬í¼ ì´ë¦„ê³¼ ìƒì„±ëœ Job File ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        # ì…ë ¥ê°’ ê²€ì¦
        gripper_name = self.validate_inputs(robot_model, gripper_type)
        # FAISS ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰
        context = self.retrieve_context(robot_model, gripper_name)
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.generate_prompt(robot_model, gripper_name, context)
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(self.device)
        print("\nğŸ”¹ Generating Job File...\n")
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            output_tokens = self.model.generate(
                input_ids=inputs["input_ids"],
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.tokenizer.pad_token_id
            )
        jobfile_response = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
        return gripper_name, jobfile_response

    def determine_file_extension(self, robot_model: str) -> str:
        """
        ë¡œë´‡ ëª¨ë¸ì— ë”°ë¼ Job File í™•ì¥ìë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        """
        return "drl" if robot_model == "H2017" else "job"


class IOHandler:
    """
    ì‚¬ìš©ì ì…ë ¥ê³¼ ì¶œë ¥, íŒŒì¼ ì €ì¥ ë“±ì˜ I/Oë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤.
    """
    @staticmethod
    def get_user_input() -> Tuple[str, str]:
        robot_model = input("ğŸ¤– Enter the robot model (H2017, HS220): ").strip().upper()
        gripper_type = input("ğŸ› ï¸ Enter the gripper type (A: Suction, B: Parallel Jaw, C: Magnetic): ").strip().upper()
        return robot_model, gripper_type

    @staticmethod
    def save_job_file(content: str, robot_model: str, gripper_type: str, file_extension: str) -> str:
        job_file_path = f"./robot_job_{robot_model}_{gripper_type}.{file_extension}"
        with open(job_file_path, "w", encoding="utf-8") as f:
            f.write(content)
        return job_file_path

    @staticmethod
    def display_results(robot_model: str, gripper_name: str, job_file_path: str, jobfile_response: str) -> None:
        print("\nğŸ”¹ **Generated Job File:**")
        print(f"ğŸ¤– Robot Model: {robot_model}")
        print(f"ğŸ› ï¸ Gripper Type: {gripper_name}")
        print(f"ğŸ“‚ Job File Saved: {job_file_path}")
        print(f"\nğŸ“œ **Generated Job File Content:**\n{jobfile_response}")


def main() -> None:
    # ì„¤ì •: ëª¨ë¸ ê²½ë¡œ, FAISS ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ, ë””ë°”ì´ìŠ¤ ì„¤ì •
    model_path = "./fine_tuned_llama_gripper_jobfile"
    db_path = "faiss_robot_gripper_jobfile_db"
    
    # Job File ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = JobFileGenerator(model_path, db_path)
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    robot_model, gripper_type = IOHandler.get_user_input()
    
    try:
        # Job File ìƒì„±
        gripper_name, jobfile_response = generator.generate_job_file(robot_model, gripper_type)
    except ValueError as e:
        print(e)
        return
    
    # íŒŒì¼ í™•ì¥ì ê²°ì • ë° Job File ì €ì¥
    file_extension = generator.determine_file_extension(robot_model)
    job_file_path = IOHandler.save_job_file(jobfile_response, robot_model, gripper_type, file_extension)
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    IOHandler.display_results(robot_model, gripper_name, job_file_path, jobfile_response)


if __name__ == "__main__":
    main()
