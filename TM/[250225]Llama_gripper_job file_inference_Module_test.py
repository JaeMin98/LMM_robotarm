import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Tuple


class JobFileGenerator:
    """
    FAISS ì—†ì´, ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Job Fileì„ ìƒì„±í•˜ëŠ” í´ëž˜ìŠ¤.
    ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ ë¡œë´‡ ëª¨ë¸ê³¼ ê·¸ë¦¬í¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Job Fileì„ ìƒˆë¡­ê²Œ ìƒì„±í•¨.
    """
    def __init__(self, model_path: str, device: str = None) -> None:
        self.device = device if device is not None else ("cuda" if torch.cuda.is_available() else "cpu")

        # Fine-Tuned ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        ).to(self.device)

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (FAISS ì—†ì´ í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ìƒì„±)")

    def generate_prompt(self, robot_model: str, gripper_name: str) -> str:
        """
        FAISS ê²€ìƒ‰ ì—†ì´, ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Job Fileì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•¨.
        """
        prompt = f"""
        Generate a Job File.

        Robot Model: {robot_model}
        Gripper Type: {gripper_name}

        Job File Format:
        {{
            "task": "Attach {gripper_name}",
            "steps": [
                {{"action": "MOVEJ", "S": 50, "A": 3, "T": 2, "coordinates": [10, 20, 30, 0, 0, 0]}},
                {{"action": "ATTACH", "gripper": "{gripper_name}"}},
                {{"action": "MOVEJ", "S": 50, "A": 3, "T": 2, "coordinates": [0, 0, 0, 0, 0, 0]}}
            ]
        }}
        Generate a new Job File in the same format based on your training data.
        """
        return prompt.strip()

    def generate_job_file(
        self,
        robot_model: str,
        gripper_name: str,
        max_new_tokens: int = 10,
        temperature: float = 0.3,
        top_p: float = 0.9,
        max_length: int = 512
    ) -> Tuple[str, str]:
        """
        FAISSë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Job Fileì„ ìƒì„±í•¨.
        """
        # ðŸ”¹ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.generate_prompt(robot_model, gripper_name)

        # ðŸ”¹ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(self.device)

        print("\nðŸ”¹ Generating Job File...\n")

        # ðŸ”¹ ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            output_tokens = self.model.generate(
                input_ids=inputs["input_ids"],
                max_new_tokens=max_new_tokens,
                do_sample=False,  # í™•ë¥ ì  ìƒ˜í”Œë§ ë¹„í™œì„±í™”
                temperature=temperature,  # ì°½ì˜ì„± ì¤„ì´ê¸°
                top_p=top_p,
                pad_token_id=self.tokenizer.pad_token_id
            )

        jobfile_response = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
        return gripper_name, jobfile_response.strip()

    def determine_file_extension(self, robot_model: str) -> str:
        """
        ë¡œë´‡ ëª¨ë¸ì— ë”°ë¼ Job File í™•ìž¥ìžë¥¼ ê²°ì •í•¨.
        """
        return "drl" if "H" in robot_model else "job"


class IOHandler:
    """
    ì‚¬ìš©ìž ìž…ë ¥ê³¼ ì¶œë ¥, íŒŒì¼ ì €ìž¥ ë“±ì˜ I/Oë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ëž˜ìŠ¤.
    """
    @staticmethod
    def get_user_input() -> Tuple[str, str]:
        robot_model = input("ðŸ¤– Enter the robot model: ").strip().upper()
        gripper_type = input("ðŸ› ï¸ Enter the gripper type: ").strip().upper()
        return robot_model, gripper_type

    @staticmethod
    def save_job_file(content: str, robot_model: str, gripper_type: str, file_extension: str) -> str:
        job_file_path = f"./robot_job_{robot_model}_{gripper_type}.{file_extension}"
        with open(job_file_path, "w", encoding="utf-8") as f:
            f.write(content)
        return job_file_path

    @staticmethod
    def display_results(robot_model: str, gripper_name: str, job_file_path: str, jobfile_response: str) -> None:
        print("\nðŸ”¹ **Generated Job File:**")
        print(f"ðŸ¤– Robot Model: {robot_model}")
        print(f"ðŸ› ï¸ Gripper Type: {gripper_name}")
        print(f"ðŸ“‚ Job File Saved: {job_file_path}")
        print(f"\nðŸ“œ **Generated Job File Content:**\n{jobfile_response}")


def main() -> None:
    model_path = "./fine_tuned_llama_gripper_jobfile"

    generator = JobFileGenerator(model_path)

    robot_model, gripper_type = IOHandler.get_user_input()

    gripper_name, jobfile_response = generator.generate_job_file(robot_model, gripper_type)

    file_extension = generator.determine_file_extension(robot_model)
    job_file_path = IOHandler.save_job_file(jobfile_response, robot_model, gripper_type, file_extension)

    IOHandler.display_results(robot_model, gripper_name, job_file_path, jobfile_response)


if __name__ == "__main__":
    main()
